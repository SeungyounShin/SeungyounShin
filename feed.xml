<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://seungyounshin.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://seungyounshin.github.io/" rel="alternate" type="text/html" hreflang="ko"/><updated>2024-01-09T23:32:12+09:00</updated><id>https://seungyounshin.github.io/feed.xml</id><title type="html">Seungyoun Shin(신승윤)</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Mixture of Experts LLM (MoE)</title><link href="https://seungyounshin.github.io/blog/2023/MoE/" rel="alternate" type="text/html" title="Mixture of Experts LLM (MoE)"/><published>2023-12-30T00:00:00+09:00</published><updated>2023-12-30T00:00:00+09:00</updated><id>https://seungyounshin.github.io/blog/2023/MoE</id><content type="html" xml:base="https://seungyounshin.github.io/blog/2023/MoE/"><![CDATA[<h2 id="moe가-왜-필요한가">MoE가 왜 필요한가?</h2> <p>Transformers 구조에서 메모리와 시간복잡도 모두 Attention 구조의 착안하여 증가한다. 그렇기 때문에 LLM에서 Self Attention 이 연산에서 큰 부분을 차지한다고 생각하기 쉽다. 하지만 <strong>실제 파라미터 수가 더 많고 연산에 꽤 많은 부분을 차지하는것은 Feed Forward Network</strong> 부분이다.</p> <p><a href="https://huggingface.co/meta-llama/Llama-2-13b">LLama2-13B</a> 의 <code class="language-plaintext highlighter-rouge">LlamaDecoderLayer</code>은 Self Attention 과 MLP (Feed Forward) 두개로 나누어져 있고 각각 파라미터 수를 계산하면 다음과 같다.</p> <table> <thead> <tr> <th style="text-align: center">LLama2-13B</th> <th style="text-align: center">LlamaAttention</th> <th style="text-align: center">LlamaMLP</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">13.02B</td> <td style="text-align: center">4.19B</td> <td style="text-align: center"><strong>8.49B</strong></td> </tr> </tbody> </table> <p>이와 같이 Llama 에서 MLP 가 Attention의 모든 query,key,value 파라미터 보다도 <strong>2배</strong> 가까이 많은것을 알 수 있다.</p> <p>즉, Attention 보다도 <strong>Feed Foward</strong> 레이어를 최적화하는것이 전체적인 throughput 향상에 도움이 될 수 있다.</p> <h2 id="mixtral-8x7b">Mixtral 8x7B</h2> <p>그렇다면, MoE 로 핫한 Mistral 은 어떤가? <a href="https://mistral.ai/news/mixtral-of-experts/">Mistral 8x7B</a> 도 똑같이 구해보자.</p> <table> <thead> <tr> <th style="text-align: center">Mistral 8x7B</th> <th style="text-align: center">MixtralAttention</th> <th style="text-align: center">MixtralMoeBlock</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">46.70B</td> <td style="text-align: center">1.34B</td> <td style="text-align: center"><strong>45.10B</strong></td> </tr> </tbody> </table> <p>Mistral 의 경우 MixtralMoeBlock 의 8개의 Experts 가 파라미터가 MistralAttention 보다 무려 <strong>33배</strong>가 많다. 실제 여기서 2개의 Expert 를 사용하니 Inference 자체에 사용되는 양은 $33/4 \approx 8$ 배 정도일것이다.</p> <p>물론 Mixtral 8x7B 는 조금 다른 방식의 Scaling 을 쓰고있다.</p> <p>우선, <a href="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws for Neural Language Models</a> 논문의 e.q:2.1 을 보면,</p> \[d_{\text{attn}} = d_{\text{ff}}/4 = d_{\text{model}}\] <p>위 와 같이 Feed Foward 의 중간 차원 $d_{\text{ff}}$ 은 전체 모델 및 Attention 의 차원 $d_{\text{attn}}, d_{\text{model}}$ 에 1/4 가 되게 설정한 채로 Scaling 을 진행한다.</p> <p>Llama2 와 Mixtral 8x7B의 $d_{\text{ff}}/d_{\text{model}}$ 를 계산해보자.</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">$d_{\text{ff}}$</th> <th style="text-align: center">$d_{\text{model}}$</th> <th style="text-align: center">$d_{\text{ff}}/d_{\text{model}}$</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://arxiv.org/pdf/2001.08361.pdf">Kaplan et al.</a></td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> <td style="text-align: center">4</td> </tr> <tr> <td style="text-align: center">Llama2 13B</td> <td style="text-align: center">13824</td> <td style="text-align: center">5120</td> <td style="text-align: center">2.7</td> </tr> <tr> <td style="text-align: center">Mixtral 8x7B</td> <td style="text-align: center">14336</td> <td style="text-align: center">4096</td> <td style="text-align: center">3.5</td> </tr> </tbody> </table> <p>하지만 Mixtral 의 경우 Query 와 Key Projection 의 Output 차원이 다르므로 Query Output Dimension 을 기준으로 계산하였다. 결과적으로는 Mixtral 도 $d_{\text{ff}}/d_{\text{model}}$ 가 3.5 로 4보다 작다.</p> <p>Llama2와 확연히 다른점은 Attention 쪽 파라미터가 더 작으면서 (Llama2-13B는 4.19B, Mixtral 8x7B 은 1.34B) Feed Forward 의 $d_{\text{ff}}$ 를 Scaling 한 것이다. 이러한 구조를 통해 Llama2 보다 더 좋은 성능을 낸다고 리포트하고 있다.</p> <p><img src="/assets/img/mixtral_perf.png" alt="Mixtral Performance" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/></p> <p>이 Figure 가 가장 전반적인 성능을 볼 수 있는 것 같아 가지고 왔다. Mixtral 8x7B는 Active Paramater 가 12B (실제 2개의 Experts만 선택을 하니까) 인데 대충 Llama13B와 비교해도 성능이 높고 70B보다도 좋다는것을 강조하고 싶었던것 같다.</p> <p>Mixtral 8x7B가 전체 Param 이 46B 정도인데 70B Llama2 보다 <strong>Code, Math 같이 Reasoning</strong> 이 많이 필요한 곳에서도 월등히 잘하는것은 결과에 조금 의구심이 들기도한다. 물론 CodeLlama 가 Code 부분에서 더 잘하는것을 생각해보면 벤치마킹을 위해 데이터셋을 보강하고 집중적으로 훈련시키면 불가능한 부분은 아니라는 생각이 들긴한다.</p> <p>몇가지 더 궁금한 점은</p> <ul> <li>우선, <strong>모든 Experts 를 다 쓰도록</strong> Inference 하면 성능이 더 떨어지는지</li> <li>Knowledge, MATH, Code … 각기 다른 벤치마크에서 MoE의 <strong>특정 Experts 가 계속 선택</strong>이 되는지가 조금 궁금하다.</li> </ul> <p>그런데 그런 실험 결과를 리포트하지 않은것을 보면 어쩌면 Experts 를 다 쓰게하면 성능이 떨어지는것이 아닐까 싶다. 혹시나 해서 🤗HF 의 <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions?status=open">discussions</a> 를 찾아보았는데 Experts 를 다 쓰는 경우에 대한 성능리포트를 대신 해준 사람은 없는것 같다.</p> <h2 id="routing-analysis">Routing Analysis</h2> <p><a href="https://arxiv.org/pdf/2401.04088.pdf">Mixtral of Experts</a> 라는 제목으로 논문이 공개가 되었고 위의 두번째 질문 (“Knowledge, MATH, Code 등 각기 다른 벤치마크에서 MoE의 <strong>특정 Experts 가 계속 선택</strong>이 되는지가 조금 궁금하다. “) 에 대해서 <strong>Routing Analysis</strong> 에서 분석하고 있어 관련 내용을 정리해보았다.</p> <p><img src="/assets/img/mixtral_fig7.png" alt="Figure 7" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"/></p> <p><a href="https://arxiv.org/abs/2101.00027">Pile</a> 데이터셋에서 전체 시퀀스를 넣고 각각 도메인 (Arxiv, Github 등등) 에서 각각 Experts 들이 얼마나 Selection 되는지를 정리한 도식이다. 회색 dash line 이 1/8 이어서 이 회색선 위에 위치한것은 그만큼 많이 선택된 Experts 라는 것이다. 수학($\texttt{DM Mathematics}$) 는 31번째 Layer 에서 압도적으로 0번 Expert 에 의해 선택된것을 확인해볼 수 있다. 반대로 똑같이 31번째 Layer 에서 언어가 좀더 많은 데이터셋인 $\texttt{StackExchange},\texttt{Wikipedia}$ 에서는 8번째 Expert가 선택된것도 재밌다. 또 0번째 Layer 에서는 비교적 균등하게 선택되다가 Layer 가 점점더 깊이 올라올 수록 특정 도메인에 특정 Expert 가 더 많이 선택되는 결과도 재밌다.</p>]]></content><author><name>신승윤</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[요즘 핫한 MoE 에 대해 알아보자.]]></summary></entry><entry><title type="html">dataclasses — 데이터 클래스</title><link href="https://seungyounshin.github.io/blog/2023/python-dataclass/" rel="alternate" type="text/html" title="dataclasses — 데이터 클래스"/><published>2023-12-23T06:01:00+09:00</published><updated>2023-12-23T06:01:00+09:00</updated><id>https://seungyounshin.github.io/blog/2023/python-dataclass</id><content type="html" xml:base="https://seungyounshin.github.io/blog/2023/python-dataclass/"><![CDATA[<p>프로그래밍에서 <code class="language-plaintext highlighter-rouge">interface</code>를 잘 설계하는 것은 코드의 가독성과 유지 보수성을 높일 수 있다.</p> <p>Python 3.7 이상에서 사용할 수 있는 <code class="language-plaintext highlighter-rouge">dataclass</code>는 데코레이터를 통해 클래스 선언을 간소화할 수 있는데 또한 <code class="language-plaintext highlighter-rouge">interface</code> 로서의 역할에도 매우 좋다. 요즘 유행하는 <a href="https://docs.pydantic.dev/latest/">pydantic</a>도 이런 dataclass 에 validation 을 할 수 있는 패키지중 하나이다.</p> <h2 id="dataclass의-기본-사용법">Dataclass의 기본 사용법</h2> <p>C++의 구조체와 유사한 방식으로, Python에서도 간단한 데이터 구조를 빠르게 정의할 수 있는데 x,y 를 가지는 <code class="language-plaintext highlighter-rouge">Point</code> 클래스의 예를 만들어보자.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Point</span><span class="p">:</span>
    <span class="n">x</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">y</span><span class="p">:</span> <span class="nb">float</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">dataclass</code> 데코레이터는 <code class="language-plaintext highlighter-rouge">__init__</code>, <code class="language-plaintext highlighter-rouge">__repr__</code>, <code class="language-plaintext highlighter-rouge">__eq__</code> 등의 메서드를 자동으로 추가해준다. 그렇기 때문에 인스턴스를 출력하면</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="nc">Point</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="nc">Point</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div> <p>다음과 같이 깔끔하게 포맷된 결과가 출력된다.</p> <h2 id="transformers-예시"><code class="language-plaintext highlighter-rouge">transformers</code> 예시</h2> <p>오픈소스 모델들을 쉽게 이용할 수 있는 레포인 <a href="https://github.com/huggingface/transformers">transformers</a> 에서도 <code class="language-plaintext highlighter-rouge">Argument</code> 와 모델의 인풋 아웃풋와 같은 interface 를 모두 <code class="language-plaintext highlighter-rouge">dataclass</code> 로 정의해서 사용하고있다.</p> <p>예를 들어 거의 모든 언어모델들의 아웃풋은 <a href="https://github.com/huggingface/transformers/blob/29e7a1e1834f331a4916853ecd58549ed78235d6/src/transformers/modeling_outputs.py#L25"><code class="language-plaintext highlighter-rouge">BaseModelOutput</code></a>를 상속해서 쓰는데 <code class="language-plaintext highlighter-rouge">BaseModelOutput</code> 를 보면</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">BaseModelOutput</span><span class="p">(</span><span class="n">ModelOutput</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
</span><span class="gp">    ...</span>
    <span class="sh">"""</span>

    <span class="n">last_hidden_state</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span>

</code></pre></div></div> <p>와 같이 <code class="language-plaintext highlighter-rouge">BaseModelOutput</code> 의 아웃풋은 보통의 <code class="language-plaintext highlighter-rouge">transformer</code> 구조에서 나올 수 있는 아웃풋들을 포함하는것을 알 수 있다. 이런식으로 Interface 를 datalcass 로 설계하면 우선 코드가 매우 간결해진다. 만약 ABC 클래스로 부터 이런 interface 를 만든다고 한다면 <code class="language-plaintext highlighter-rouge">__init__</code> 부터 여러가지 설계해야할점이 너무나 많고 코드가 방대해진다.</p>]]></content><author><name></name></author><category term="python"/><summary type="html"><![CDATA[an example of a blog post with some code]]></summary></entry></feed>