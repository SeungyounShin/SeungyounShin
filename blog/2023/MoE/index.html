<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Mixture of Experts LLM (MoE) | Seungyoun Shin(신승윤)</title> <meta name="author" content="Seungyoun Shin"> <meta name="description" content="요즘 핫한 MoE 에 대해 알아보자."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&amp;display=swap" rel="stylesheet"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/colorful.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://seungyounshin.github.io/blog/2023/MoE/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Mixture of Experts LLM (MoE)",
      "description": "요즘 핫한 MoE 에 대해 알아보자.",
      "published": "December 30, 2023",
      "authors": [
        {
          "author": "신승윤",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-bolder" href="https://SeungyounShin.github.io/">Seungyoun Shin(신승윤)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Mixture of Experts LLM (MoE)</h1> <p>요즘 핫한 MoE 에 대해 알아보자.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#moe%EA%B0%80-%EC%99%9C-%ED%95%84%EC%9A%94%ED%95%9C%EA%B0%80">MoE가 왜 필요한가?</a></div> <div><a href="#mixtral-8x7b">Mixtral 8x7B</a></div> <div><a href="#routing-analysis">Routing Analysis</a></div> </nav> </d-contents> <h2 id="moe가-왜-필요한가">MoE가 왜 필요한가?</h2> <p>Transformers 구조에서 메모리와 시간복잡도 모두 Attention 구조의 착안하여 증가한다. 그렇기 때문에 LLM에서 Self Attention 이 연산에서 큰 부분을 차지한다고 생각하기 쉽다. 하지만 <strong>실제 파라미터 수가 더 많고 연산에 꽤 많은 부분을 차지하는것은 Feed Forward Network</strong> 부분이다.</p> <p><a href="https://huggingface.co/meta-llama/Llama-2-13b" rel="external nofollow noopener" target="_blank">LLama2-13B</a> 의 <code class="language-plaintext highlighter-rouge">LlamaDecoderLayer</code>은 Self Attention 과 MLP (Feed Forward) 두개로 나누어져 있고 각각 파라미터 수를 계산하면 다음과 같다.</p> <table> <thead> <tr> <th style="text-align: center">LLama2-13B</th> <th style="text-align: center">LlamaAttention</th> <th style="text-align: center">LlamaMLP</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">13.02B</td> <td style="text-align: center">4.19B</td> <td style="text-align: center"><strong>8.49B</strong></td> </tr> </tbody> </table> <p>이와 같이 Llama 에서 MLP 가 Attention의 모든 query,key,value 파라미터 보다도 <strong>2배</strong> 가까이 많은것을 알 수 있다.</p> <p>즉, Attention 보다도 <strong>Feed Foward</strong> 레이어를 최적화하는것이 전체적인 throughput 향상에 도움이 될 수 있다.</p> <h2 id="mixtral-8x7b">Mixtral 8x7B</h2> <p>그렇다면, MoE 로 핫한 Mistral 은 어떤가? <a href="https://mistral.ai/news/mixtral-of-experts/" rel="external nofollow noopener" target="_blank">Mistral 8x7B</a> 도 똑같이 구해보자.</p> <table> <thead> <tr> <th style="text-align: center">Mistral 8x7B</th> <th style="text-align: center">MixtralAttention</th> <th style="text-align: center">MixtralMoeBlock</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">46.70B</td> <td style="text-align: center">1.34B</td> <td style="text-align: center"><strong>45.10B</strong></td> </tr> </tbody> </table> <p>Mistral 의 경우 MixtralMoeBlock 의 8개의 Experts 가 파라미터가 MistralAttention 보다 무려 <strong>33배</strong>가 많다. 실제 여기서 2개의 Expert 를 사용하니 Inference 자체에 사용되는 양은 $33/4 \approx 8$ 배 정도일것이다.</p> <p>물론 Mixtral 8x7B 는 조금 다른 방식의 Scaling 을 쓰고있다.</p> <p>우선, <a href="https://arxiv.org/pdf/2001.08361.pdf" rel="external nofollow noopener" target="_blank">Scaling Laws for Neural Language Models</a> 논문의 e.q:2.1 을 보면,</p> \[d_{\text{attn}} = d_{\text{ff}}/4 = d_{\text{model}}\] <p>위 와 같이 Feed Foward 의 중간 차원 $d_{\text{ff}}$ 은 전체 모델 및 Attention 의 차원 $d_{\text{attn}}, d_{\text{model}}$ 에 1/4 가 되게 설정한 채로 Scaling 을 진행한다.</p> <p>Llama2 와 Mixtral 8x7B의 $d_{\text{ff}}/d_{\text{model}}$ 를 계산해보자.</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">$d_{\text{ff}}$</th> <th style="text-align: center">$d_{\text{model}}$</th> <th style="text-align: center">$d_{\text{ff}}/d_{\text{model}}$</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://arxiv.org/pdf/2001.08361.pdf" rel="external nofollow noopener" target="_blank">Kaplan et al.</a></td> <td style="text-align: center">-</td> <td style="text-align: center">-</td> <td style="text-align: center">4</td> </tr> <tr> <td style="text-align: center">Llama2 13B</td> <td style="text-align: center">13824</td> <td style="text-align: center">5120</td> <td style="text-align: center">2.7</td> </tr> <tr> <td style="text-align: center">Mixtral 8x7B</td> <td style="text-align: center">14336</td> <td style="text-align: center">4096</td> <td style="text-align: center">3.5</td> </tr> </tbody> </table> <p>하지만 Mixtral 의 경우 Query 와 Key Projection 의 Output 차원이 다르므로 Query Output Dimension 을 기준으로 계산하였다. 결과적으로는 Mixtral 도 $d_{\text{ff}}/d_{\text{model}}$ 가 3.5 로 4보다 작다.</p> <p>Llama2와 확연히 다른점은 Attention 쪽 파라미터가 더 작으면서 (Llama2-13B는 4.19B, Mixtral 8x7B 은 1.34B) Feed Forward 의 $d_{\text{ff}}$ 를 Scaling 한 것이다. 이러한 구조를 통해 Llama2 보다 더 좋은 성능을 낸다고 리포트하고 있다.</p> <p><img src="/assets/img/mixtral_perf.png" alt="Mixtral Performance" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"></p> <p>이 Figure 가 가장 전반적인 성능을 볼 수 있는 것 같아 가지고 왔다. Mixtral 8x7B는 Active Paramater 가 12B (실제 2개의 Experts만 선택을 하니까) 인데 대충 Llama13B와 비교해도 성능이 높고 70B보다도 좋다는것을 강조하고 싶었던것 같다.</p> <p>Mixtral 8x7B가 전체 Param 이 46B 정도인데 70B Llama2 보다 <strong>Code, Math 같이 Reasoning</strong> 이 많이 필요한 곳에서도 월등히 잘하는것은 결과에 조금 의구심이 들기도한다. 물론 CodeLlama 가 Code 부분에서 더 잘하는것을 생각해보면 벤치마킹을 위해 데이터셋을 보강하고 집중적으로 훈련시키면 불가능한 부분은 아니라는 생각이 들긴한다.</p> <p>몇가지 더 궁금한 점은</p> <ul> <li>우선, <strong>모든 Experts 를 다 쓰도록</strong> Inference 하면 성능이 더 떨어지는지</li> <li>Knowledge, MATH, Code … 각기 다른 벤치마크에서 MoE의 <strong>특정 Experts 가 계속 선택</strong>이 되는지가 조금 궁금하다.</li> </ul> <p>그런데 그런 실험 결과를 리포트하지 않은것을 보면 어쩌면 Experts 를 다 쓰게하면 성능이 떨어지는것이 아닐까 싶다. 혹시나 해서 🤗HF 의 <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions?status=open" rel="external nofollow noopener" target="_blank">discussions</a> 를 찾아보았는데 Experts 를 다 쓰는 경우에 대한 성능리포트를 대신 해준 사람은 없는것 같다.</p> <h2 id="routing-analysis">Routing Analysis</h2> <p><a href="https://arxiv.org/pdf/2401.04088.pdf" rel="external nofollow noopener" target="_blank">Mixtral of Experts</a> 라는 제목으로 논문이 공개가 되었고 위의 두번째 질문 (“Knowledge, MATH, Code 등 각기 다른 벤치마크에서 MoE의 <strong>특정 Experts 가 계속 선택</strong>이 되는지가 조금 궁금하다. “) 에 대해서 <strong>Routing Analysis</strong> 에서 분석하고 있어 관련 내용을 정리해보았다.</p> <p><img src="/assets/img/mixtral_fig7.png" alt="Figure 7" style="display: block; margin-left: auto; margin-right: auto; width: 80%;"></p> <p><a href="https://arxiv.org/abs/2101.00027" rel="external nofollow noopener" target="_blank">Pile</a> 데이터셋에서 전체 시퀀스를 넣고 각각 도메인 (Arxiv, Github 등등) 에서 각각 Experts 들이 얼마나 Selection 되는지를 정리한 도식이다. 회색 dash line 이 1/8 이어서 이 회색선 위에 위치한것은 그만큼 많이 선택된 Experts 라는 것이다. 수학($\texttt{DM Mathematics}$) 는 31번째 Layer 에서 압도적으로 0번 Expert 에 의해 선택된것을 확인해볼 수 있다. 반대로 똑같이 31번째 Layer 에서 언어가 좀더 많은 데이터셋인 $\texttt{StackExchange},\texttt{Wikipedia}$ 에서는 8번째 Expert가 선택된것도 재밌다. 또 0번째 Layer 에서는 비교적 균등하게 선택되다가 Layer 가 점점더 깊이 올라올 수록 특정 도메인에 특정 Expert 가 더 많이 선택되는 결과도 재밌다.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Seungyoun Shin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>