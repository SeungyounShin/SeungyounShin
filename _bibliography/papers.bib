---
---


@inproceedings{shin2023past,
title={Past as a Guide: Leveraging Retrospective Learning for Python Code Completion},
author={Seungyoun Shin and Seunggyu Chang and Sungjoon Choi},
booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
year={2023},
abstract={
This work presents Past as a Guide (PaG), a simple approach for Large Language Models (LLMs) to improve the coding capabilities by integrating the past history with interactive and iterative code refinements. To be specific, inspired by human cognitive processes, the proposed method enables LLMs to utilize previous programming and debugging experiences to enhance the Python code completion tasks. The framework facilitates LLMs to iteratively refine the Python code based on previous execution and debugging results and optimize learning and reasoning capabilities. The proposed methodology achieved a 92% pass@1 on HumanEval, demonstrating the potential to advance the field by leveraging retrospection from past experiences and interactive and iterative refinement processes without external correctness indicators.
},
pdf={https://arxiv.org/abs/2311.07635},
code={https://github.com/SeungyounShin/Past-as-a-Guide},
blog={https://seungyounshin.github.io/Past-as-a-Guide},
abbr={NeurIPS},
award={Workshop},
url={https://openreview.net/forum?id=G8ArB0aApM},
selected={true}
}

@inproceedings{shin2023ucam,
title={Towards Explainable Computer Vision Methods via Uncertainty Activation Map},
author={Shin, Seungyoun and Bae, Wonho and Noh, Junhyug and Choi, Sungjoon},
booktitle={ACPR 2023:Pattern Recognition},
year={2023},
abstract={This paper focuses on the problem of highlighting the input image regions that result in increasing predictive uncertainty. In particular, we focus on two types of uncertainty, epistemic and aleatoric, and present an uncertainty activation mapping method that can incorporate both types of uncertainty. To this end, we first utilize a mixture-of-experts model combined with class-activation mapping (CAM). The proposed method is extensively evaluated in two different scenarios: multi-label and artificial noise injection scenarios, where we show that our proposed method can effectively capture uncertain regions},
abbr={ACPR2023},
url={https://link.springer.com/chapter/10.1007/978-3-031-47634-1_1#citeas},
selected={true}
}

@inproceedings{shin2023gaemi,
title={Robust Detection for Autonomous Elevator Boarding Using a Mobile Manipulator},
author={Shin, Seungyoun and Lee, Joon Hyung and Noh, Junhyug and Choi, Sungjoon},
booktitle={ACPR 2023:Pattern Recognition},
year={2023},
abstract={Indoor robots are becoming increasingly prevalent across a range of sectors, but the challenge of navigating multi-level structures through elevators remains largely uncharted. For a robot to operate successfully, it’s pivotal to have an accurate perception of elevator states. This paper presents a robust robotic system, tailored to interact adeptly with elevators by discerning their status, actuating buttons, and boarding seamlessly. Given the inherent issues of class imbalance and limited data, we utilize the YOLOv7 model and adopt specific strategies to counteract the potential decline in object detection performance. Our method effectively confronts the class imbalance and label dependency observed in real-world datasets, Our method effectively confronts the class imbalance and label dependency observed in real-world datasets, offering a promising approach to improve indoor robotic navigation systems.},
abbr={ACPR2023},
url={https://link.springer.com/chapter/10.1007/978-3-031-47634-1_2},
selected={true}
}


@article{kim2022learning,
  title={Learning Joint Representation of Human Motion and Language},
  author={Jihoon Kim and Youngjae Yu and Seungyoun Shin and Taehyun Byun and Sungjoon Choi},
  journal={arXiv:2210.15187},
  abbr={arXiv},
  pdf={https://arxiv.org/pdf/2210.15187},
  year={2022},
  selected={true}
}

@article{park2023elucidating,
  title={Elucidating robust learning with uncertainty-aware corruption pattern estimation},
  author={Jeongeun Park and Seungyoun Shin and Sangheum Hwang and Sungjoon Choi},
  journal={Pattern Recognition},
  abbr={PR},
  pdf={https://arxiv.org/abs/2111.01632},
  year={2022},
  selected={true}
}

